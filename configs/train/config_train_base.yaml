# base: google/vit-base-patch16-224-in21k
# - https://huggingface.co/aaraki/vit-base-patch16-224-in21k-finetuned-cifar10 aaraki/vit-base-patch16-224-in21k-finetuned-cifar10 (vision)
# - https://huggingface.co/microsoft/swin-large-patch4-window7-224-in22k microsoft/swin-large-patch4-window7-224-in22k (vision)
# - https://huggingface.co/openai/clip-vit-base-patch16 openai/clip-vit-base-patch16 (vision)
# - https://huggingface.co/nlpconnect/vit-gpt2-image-captioning nlpconnect/vit-gpt2-image-captioning (multimodal)
# - https://huggingface.co/beomi/kykim-gpt3-kor-small_based_on_gpt2 beomi/kykim-gpt3-kor-small_based_on_gpt2 (text)

model_name: "VIT-GPT2"

model_config:
  vision_encoder: "google/vit-base-patch16-224-in21k" 
  text_decoder: "skt/kogpt2-base-v2"

data_config:
  base_dir: "/data/swbaek/Projects/Korean_IC_Competition/data"
  image_dir: "nikluge-gips-2023_image"
  jsonl_dir: "nikluge-gips-2023_JSONL"
  train_file: "nikluge-gips-2023-train.jsonl"
  val_file: "nikluge-gips-2023-dev.jsonl"

common_parameters:
  output_dir: "outputs/${model_name}"
  model_path: null
  batch_size: 32
  valid_batch_size: 32
  accumulate_grad_batches: 8
  epochs: 2
  max_learning_rate: 0.0002
  min_learning_rate: 0.00001
  warmup_rate: 0.1
  gpus: 8
  logging_interval: 100
  evaluate_interval: 1.0
  seed: 42
  num_proc: 32
  lr_scheduler_type: "cyclic"

  cosine_annealing_WR:
    T_0: 1
    T_mult: 2

wandb_options:
  wandb_run_name: null
  wandb_entity: null
  wandb_project: "${model_name}"
  wandb_save_dir: "outputs/${model_name}/wandb_logs"
